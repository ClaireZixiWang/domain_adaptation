{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578648c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.utils.data as torchdata\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "# from train_model import train_model\n",
    "# from test_model import test_model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3abfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='3'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50bef3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "american-flag\n"
     ]
    }
   ],
   "source": [
    "filePath = '/local/rcs/ll3504/datasets/256_ObjectCategories/'\n",
    "namelist = os.listdir(filePath)\n",
    "nameDic_cal = {}\n",
    "for name in namelist:\n",
    "    splits = name.split(\".\")\n",
    "    nameDic_cal[int(splits[0])-1] = splits[1]\n",
    "print(nameDic_cal[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af7d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path='/database', dataset_name='caltech-256-common'):\n",
    "    # No holdout testing data. train and test data are the same, but different transformation\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "#             transforms.Resize([256, 256]),\n",
    "#             transforms.RandomCrop(224),\n",
    "#             transforms.RandomRotation(20),\n",
    "#             transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "#             transforms.Resize([224, 224]),\n",
    "#             transforms.CenterCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    tr_dataset = datasets.ImageFolder(path + dataset_name + '/', data_transforms['train'])\n",
    "    te_dataset = datasets.ImageFolder(path + dataset_name + '/', data_transforms['test'])\n",
    "#     print('{} train set size: {}'.format(dataset_name, len(tr_dataset)))\n",
    "#     print('{} test set size: {}'.format(dataset_name, len(te_dataset)))\n",
    "\n",
    "    return tr_dataset, te_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42856c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(train_dataset, test_dataset, valid_size=0.2, batch_size=128, train_size = 128):\n",
    "    '''\n",
    "    This function splits dataset into train, val, and test sets, and return train, val, test dataloaders.\n",
    "    Val and Test loaders are the same\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # what does the len function gives?\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    random.shuffle(indices)\n",
    "    train_idx, valid_idx = indices[split:split+train_size], indices[:split]\n",
    "#     print(\"DEBUGGING: train_idx =\", train_idx, \"valid_idx =\", valid_idx)\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "#     print(\"DEBUGGING: the train_ind are:\", len(train_idx))\n",
    "\n",
    "\n",
    "    train_loader = torchdata.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=48, pin_memory=True, drop_last=True, sampler = train_sampler)\n",
    "    test_loader = torchdata.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=48, pin_memory=True, drop_last=True, sampler = valid_sampler)\n",
    "    dataloaders = {'train': train_loader,\n",
    "                   'val': test_loader,\n",
    "                   'test': test_loader}\n",
    "    dataset_sizes ={'train': train_size, #int(np.floor((1-valid_size) * num_train)),\n",
    "                    'val': int(np.floor(valid_size * num_train)),\n",
    "                    'test': int(np.floor(valid_size * num_train))}\n",
    "    return dataloaders, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4078f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagebase = '/local/rcs/ll3504/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f0d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "corruption = ['zoom_blur', 'speckle_noise', 'spatter',\n",
    "                       'snow', 'glass_blur', 'motion_blur', 'saturate',\n",
    "                       'gaussian_blur', 'frost', 'fog', 'brightness', 'contrast',\n",
    "                       'elastic_transform', 'pixelate', 'jpeg_compression', 'defocus_blur']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77838a15",
   "metadata": {},
   "source": [
    "# ImageNetC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b8850d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imagenetc(imagebase, severity=1, batch_size=128, sample_size = 128):\n",
    "    '''\n",
    "    Returns:\n",
    "        ref_dataloaders:          ImageNet original validation data, as a reference\n",
    "        ref_dataset_sizes:        1000, not the sizes of the real dataset in the ref_loader, probs used downstream\n",
    "        corrupted_dataloaders:    A list of corrupted dataloaders, each element in a list represetns the data loaders\n",
    "                                  for one corruption type. Each element contains ['train']['val']['test'] loaders\n",
    "        corrupted_dataset_sizes:  A list of dictionaries of the sizes of each loaders for each corruption\n",
    "        corruption:               A list of corruption names, in the same order of the corrupted_dataloaders\n",
    "    '''\n",
    "    corruption = ['zoom_blur', 'speckle_noise', 'spatter',\n",
    "                       'snow', 'glass_blur', 'motion_blur', 'saturate',\n",
    "                       'gaussian_blur', 'frost', 'fog', 'brightness', 'contrast',\n",
    "                       'elastic_transform', 'pixelate', 'jpeg_compression', 'defocus_blur']\n",
    "    corrupted_dataloaders = []\n",
    "    corrupted_dataset_sizes = []\n",
    "    \n",
    "    # this is the imageNet validation data\n",
    "    imagenet_val = datasets.ImageNet(imagebase+'imagenetc/', split='val', transform=transforms.Compose([\n",
    "            transforms.Resize([224, 224]),\n",
    "            transforms.CenterCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "                                   target_transform=None)#, download=False)\n",
    "    \n",
    "    # TODO: subsample some size of ImageNet training data as source\n",
    "        # Doesn't need this step\n",
    "#     print(\"DEBUGGING: imagenet_val size is:\", len(imagenet_val))\n",
    "    \n",
    "    random_indices = random.sample(range(0, len(imagenet_val)), int(len(imagenet_val)*0.02))\n",
    "#     print(\"DEBUGGING: random indices are:\", len(random_indices))\n",
    "    imagenet_val_subset = data.Subset(imagenet_val, random_indices)\n",
    "    val_loader = torch.utils.data.DataLoader(imagenet_val_subset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=48)\n",
    "    ref_dataloaders = { 'val': val_loader,\n",
    "                       'test': val_loader}\n",
    "    ref_dataset_sizes ={'val': int(len(val_loader.dataset)),\n",
    "                        'test': int(len(val_loader.dataset))}\n",
    "    \n",
    "    # for every type of corruption, go to the specified severity folder\n",
    "    for corr in corruption:\n",
    "        dataset_name = 'imagenetc/' + corr + '/' + str(severity)\n",
    "        # Get dataset from folder\n",
    "        corr_trian_images, corr_test_images = get_dataset(imagebase, dataset_name)\n",
    "        # get corruption-specific train, val, test loader\n",
    "        # train: training data, non-overlap with val/test\n",
    "        # val: non-overlap with train, same as test\n",
    "        # test: non-overlap with train, same as test\n",
    "        corr_dataloaders, corr_dataset_sizes = split_dataset(corr_trian_images, corr_test_images, valid_size=0.02, batch_size=batch_size, train_size=sample_size)\n",
    "        corrupted_dataloaders.append(corr_dataloaders)\n",
    "        corrupted_dataset_sizes.append(corr_dataset_sizes)\n",
    "    return ref_dataloaders, ref_dataset_sizes, corrupted_dataloaders, corrupted_dataset_sizes, corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d850d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ref_dataloaders, ref_dataset_sizes, corrupted_dataloaders, corrupted_dataset_sizes, corruption \\\n",
    "= get_imagenetc(imagebase, 5, 32, 64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd812a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in total 16 corruptions, each have 3 dataloaders ['train']['val']['test']\n",
    "len(corrupted_dataloaders[15]['val']) # why is this 31??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d72ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_dataset_sizes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dataset_sizes['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b591be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_dataloaders['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34810f8",
   "metadata": {},
   "source": [
    "# Vanila Resnet-50 Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8000283",
   "metadata": {},
   "source": [
    "Test top-1 accuracy for all corruptions, for different batch sizes, average accuracy over batches, (inference time)\n",
    "on reference data (IM-val) and corruption test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e3c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsizes = [32, 64, 128, 256, 512, 1000]\n",
    "severity = [1,2,3,4,5]\n",
    "# a dictionary that maps 'coruption_type' to a table of 5x6, \n",
    "# recording the performance of all corruption for all batchsizes\n",
    "# baseline_performances = {}\n",
    "\n",
    "# # in addition to all the corruptions, save performance for reference (IM-val) too\n",
    "# baseline_performances['ref'] = np.zeros(shape=(len(severity), len(batchsizes)))\n",
    "# for c in corruption:\n",
    "#     baseline_performances[c] = np.zeros(shape=(len(severity), len(batchsizes)))\n",
    "\n",
    "baseline_performances = np.zeros(shape=(len(corruption), len(severity), len(batchsizes)))\n",
    "baseline_inf_times = np.zeros(shape=(len(corruption), len(severity), len(batchsizes)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0690b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bcf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for severity_ind in range(len(severity)):\n",
    "    for bs_ind in range(len(batchsizes)):\n",
    "        ref_dataloaders, ref_dataset_sizes, corrupted_dataloaders, corrupted_dataset_sizes, corruption \\\n",
    "        = get_imagenetc(imagebase, severity[severity_ind], batchsizes[bs_ind])\n",
    "        for cor_ind in range(len(corrupted_dataloaders)):\n",
    "            with torch.no_grad():\n",
    "                # TOOD: also record average batch inference time.\n",
    "                batch_accs = []\n",
    "                batch_inf_times = []\n",
    "                for data in corrupted_dataloaders[cor_ind]['test']:\n",
    "                    start = time.time()\n",
    "                    images, labels = data[0].to(device), data[1].to(device)\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    inference_time = time.time() - start\n",
    "                    batch_inf_times.append(inference_time)\n",
    "                    batch_accuracy = ((predicted == labels).sum().item()) / labels.size(0)\n",
    "                    batch_accs.append(batch_accuracy)\n",
    "            print(f'Vanilla resnet50 on 1000 test images of corruption {corruption[cor_ind]} with severity {severity[severity_ind]} and batch_size {batchsizes[bs_ind]}, average batch_size = {np.mean(batch_accs)}')\n",
    "            baseline_performances[cor_ind, severity_ind, bs_ind] = np.mean(batch_accs)\n",
    "            print(f'Average Inference time = {np.mean(batch_inf_times)}')\n",
    "            baseline_inf_times[cor_ind, severity_ind, bs_ind] = np.mean(batch_inf_times)\n",
    "            \n",
    "        with open('ResNet50_performances.npy', 'wb') as f:\n",
    "            np.save(f, baseline_performances)\n",
    "        with open('ResNet50_times.npy', 'wb') as f:\n",
    "            np.save(f, baseline_inf_times)\n",
    "        \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b29497",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# baseline_performances\n",
    "baseline_inf_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294670c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_inf_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dcf5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ResNet50_performances.npy', 'wb') as f:\n",
    "    np.save(f, baseline_performances)\n",
    "with open('ResNet50_times.npy', 'wb') as f:\n",
    "    np.save(f, baseline_inf_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a4cbab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('ResNet50_performances.npy', 'rb') as f:\n",
    "    baseline_performances = np.load(f)\n",
    "with open('ResNet50_times.npy', 'rb') as f:\n",
    "    baseline_inf_times = np.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0429680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6334677419354839"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-baseline_performances[0,:,0].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be24fe9",
   "metadata": {},
   "source": [
    "## Graphing the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a0f57",
   "metadata": {},
   "source": [
    "# Robust Pseudo label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49388ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5dfa81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "def gce(logits, target, q = 0.8):\n",
    "    \"\"\" Generalized cross entropy.\n",
    "    \n",
    "    Reference: https://arxiv.org/abs/1805.07836\n",
    "    \"\"\"\n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    probs_with_correct_idx = probs.index_select(-1, target).diag()\n",
    "    loss = (1. - probs_with_correct_idx**q) / q\n",
    "    return loss.mean()\n",
    "\n",
    "def adapt_batchnorm(model):\n",
    "    model.eval()\n",
    "    parameters = []\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.BatchNorm2d):\n",
    "            parameters.extend(module.parameters())\n",
    "            module.train()\n",
    "    return parameters\n",
    "\n",
    "\n",
    "# ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt(\n",
    "#         datadir = '/data/imagenetc/gaussian_blur/3',\n",
    "        baseline,\n",
    "        model,\n",
    "        dataloader,\n",
    "        num_epochs = 1, # followed their findings in the paper\n",
    "        batch_size = 32,\n",
    "        learning_rate = 1e-3,\n",
    "        gce_q = 0.8,\n",
    "    ):\n",
    "    \n",
    "    # model = models.resnet50(pretrained = True).to(device)\n",
    "    parameters = adapt_batchnorm(model)\n",
    "    \n",
    "    # TODO change this loader\n",
    "    # val_loader = get_dataset_loader(\n",
    "    #    datadir,\n",
    "    #    batch_size = batch_size,\n",
    "    #    shuffle = True\n",
    "    # )\n",
    "    \n",
    "    optimizer = torch.optim.SGD(\n",
    "        parameters, lr = learning_rate\n",
    "    )\n",
    "    \n",
    "    b_correct, num_correct, num_samples = 0., 0., 0.\n",
    "    for epoch in range(num_epochs):\n",
    "        predictions = []\n",
    "        batch_accs = []\n",
    "        for images, labels in dataloader:\n",
    "            # start = time.time()\n",
    "            \n",
    "            outputs = model(images.to(device))\n",
    "            predictions = outputs.argmax(dim = 1)\n",
    "            # _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # inference_time = time.time() - start\n",
    "            \n",
    "            # TODO: in our scenario, do we want to revert back to original model after adapting in each step?\n",
    "            loss = gce(outputs, predictions, q = gce_q)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            b_outputs = baseline(images.to(device))\n",
    "            b_predictions = b_outputs.argmax(dim = 1)\n",
    "\n",
    "            num_correct += (predictions.detach().cpu() == labels).float().sum()\n",
    "            b_correct += (b_predictions.detach().cpu() == labels).float().sum()\n",
    "\n",
    "            num_samples += len(labels)\n",
    "\n",
    "            print(f\"Baseline Correct: {b_correct:#5.0f}/{num_samples:#5.0f} ({100 * b_correct / num_samples:.2f} %)\")\n",
    "            print(f\"Adapt Correct: {num_correct:#5.0f}/{num_samples:#5.0f} ({100 * num_correct / num_samples:.2f} %)\")\n",
    "    \n",
    "    return num_correct/num_samples, b_correct/num_samples\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b41912",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sizes = [32, 64, 128, 256, 512]\n",
    "severity = [1,2,3,4,5]\n",
    "\n",
    "rpl_corrupt_train_acc = np.zeros(shape=(len(corruption), len(severity), len(target_sizes)))\n",
    "rpl_corrupt_val_acc = np.zeros(shape=(len(corruption), len(severity), len(target_sizes)))\n",
    "rpl_ref_acc = np.zeros(shape=(len(corruption), len(severity), len(target_sizes)))\n",
    "\n",
    "\n",
    "# rpl_adpt_times = np.zeros(shape=(len(corruption), len(severity), len(target_sizes)))\n",
    "baseline_train_acc = np.zeros(shape=(len(corruption), len(severity), len(target_sizes)))\n",
    "baseline_val_acc = np.zeros(shape=(len(corruption), len(severity), len(target_sizes)))\n",
    "baseline_ref_acc = np.zeros(shape=(len(corruption), len(severity), len(target_sizes)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2798900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = models.resnet50(pretrained=True)\n",
    "baseline.to(device)\n",
    "baseline.eval()\n",
    "for severity_ind in range(len(severity)):\n",
    "    for ts_ind in range(len(target_sizes)):\n",
    "        ref_dataloaders, ref_dataset_sizes, corrupted_dataloaders, corrupted_dataset_sizes, corruption \\\n",
    "        = get_imagenetc(imagebase, severity[severity_ind], 32, target_sizes[ts_ind])\n",
    "        \n",
    "        # adapt the model using the rpl methods\n",
    "        # evaluate the adapted model on the test corrupted (target) dataset & the reference (source) dataset\n",
    "        for cor_ind in range(len(corrupted_dataloaders)):\n",
    "            \n",
    "            print(f'## Experiment: Severity = {severity[severity_ind]}, target_size = {target_sizes[ts_ind]}, corruption = {corruption[cor_ind]}')\n",
    "            model = models.resnet50(pretrained=True)\n",
    "            model.to(device)\n",
    "            \n",
    "            # adapt the model\n",
    "            # start = time.time()\n",
    "            train_acc, base_acc = adapt(baseline, model, corrupted_dataloaders[cor_ind]['train'])\n",
    "            # adapt_time = time.time() - start\n",
    "            # print(f\"Adaptation time: {adapt_time}\")\n",
    "            \n",
    "            # rpl_adpt_times[cor_ind, severity_ind, ts_ind] = adapt_time\n",
    "            rpl_corrupt_train_acc[cor_ind, severity_ind, ts_ind] = train_acc\n",
    "            baseline_train_acc[cor_ind, severity_ind, ts_ind] = base_acc\n",
    "            \n",
    "            # evaluate both the adapted model & baseline model on the, corrupted val data, and reference (soure) data\n",
    "            # In validation step I treat the method as offline adaptation, then set BN layers to eval()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                base_val_correct, crpt_val_correct, crpt_val_samples, base_ref_correct, ref_correct, ref_samples = 0, 0, 0, 0, 0, 0\n",
    "                for images, labels in corrupted_dataloaders[cor_ind]['val']:\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(images.to(device))\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    crpt_val_correct += (predicted == labels).sum().item()\n",
    "                    crpt_val_samples += len(labels)\n",
    "                    \n",
    "                    b_outputs = baseline(images.to(device))\n",
    "                    _, b_predicted = torch.max(b_outputs.data, 1)\n",
    "                    base_val_correct += (b_predicted == labels).sum().item()\n",
    "                    \n",
    "                    \n",
    "                print(f\"Corrupt Val Adapt Accuracy: {crpt_val_correct:#5.0f}/{crpt_val_samples:#5.0f} ({crpt_val_correct / crpt_val_samples})\")\n",
    "                print(f\"Corrupt Val Base Accuracy: {base_val_correct:#5.0f}/{crpt_val_samples:#5.0f} ({base_val_correct / crpt_val_samples})\")\n",
    "                \n",
    "                rpl_corrupt_val_acc[cor_ind, severity_ind, ts_ind] = crpt_val_correct / crpt_val_samples\n",
    "                baseline_val_acc[cor_ind, severity_ind, ts_ind] = base_val_correct / crpt_val_samples\n",
    "                \n",
    "\n",
    "                for images, labels in ref_dataloaders['val']:\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(images.to(device))\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    ref_correct += (predicted == labels).sum().item()\n",
    "                    ref_samples += len(labels)\n",
    "                    \n",
    "                    b_outputs = baseline(images.to(device))\n",
    "                    _, b_predicted = torch.max(b_outputs.data, 1)\n",
    "                    base_ref_correct += (b_predicted == labels).sum().item()\n",
    "                    \n",
    "                print(f\"Ref Adapt Accuracy: {ref_correct:#5.0f}/{ref_samples:#5.0f} ({ref_correct / ref_samples})\")\n",
    "                print(f\"Ref Base Accuracy: {base_ref_correct:#5.0f}/{ref_samples:#5.0f} ({base_ref_correct / ref_samples})\")\n",
    "                \n",
    "                rpl_ref_acc[cor_ind, severity_ind, ts_ind] = ref_correct / ref_samples\n",
    "                baseline_ref_acc[cor_ind, severity_ind, ts_ind] = base_ref_correct / ref_samples\n",
    "\n",
    "\n",
    "        with open('RPL_corrupt_train_acc.npy', 'wb') as f:\n",
    "            np.save(f, rpl_corrupt_train_acc)\n",
    "        with open('RPL_corrupt_validation_acc.npy', 'wb') as f:\n",
    "            np.save(f, rpl_corrupt_val_acc)\n",
    "        with open('RPL_source_acc.npy', 'wb') as f:\n",
    "            np.save(f, rpl_ref_acc)\n",
    "            \n",
    "        with open('Base_corrupt_train_acc.npy', 'wb') as f:\n",
    "            np.save(f, baseline_train_acc)\n",
    "        with open('Base_corrupt_validation_acc.npy', 'wb') as f:\n",
    "            np.save(f, baseline_val_acc)\n",
    "        with open('Base_source_acc.npy', 'wb') as f:\n",
    "            np.save(f, baseline_ref_acc)\n",
    "#         with open('RPL_adapt_time.npy', 'wb') as f:\n",
    "#             np.save(f, rpl_adpt_times)\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RPL_corrupt_train_acc.npy', 'wb') as f:\n",
    "    np.save(f, rpl_corrupt_train_acc)\n",
    "with open('RPL_corrupt_validation_acc.npy', 'wb') as f:\n",
    "    np.save(f, rpl_corrupt_val_acc)\n",
    "with open('RPL_source_acc.npy', 'wb') as f:\n",
    "    np.save(f, rpl_ref_acc)\n",
    "\n",
    "with open('Base_corrupt_train_acc.npy', 'wb') as f:\n",
    "    np.save(f, baseline_train_acc)\n",
    "with open('Base_corrupt_validation_acc.npy', 'wb') as f:\n",
    "    np.save(f, baseline_val_acc)\n",
    "with open('Base_source_acc.npy', 'wb') as f:\n",
    "    np.save(f, baseline_ref_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88346f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RPL_corrupt_train_acc.npy', 'rb') as f:\n",
    "    rpl_corrupt_train_acc = np.load(f)\n",
    "with open('RPL_corrupt_validation_acc.npy', 'rb') as f:\n",
    "    rpl_corrupt_val_acc = np.load(f)\n",
    "with open('RPL_target_acc.npy', 'rb') as f:\n",
    "    rpl_ref_acc = np.load(f)\n",
    "# with open('RPL_adapt_time.npy', 'rb') as f:\n",
    "#     rpl_adpt_times = np.load(f)\n",
    "    \n",
    "with open('Base_corrupt_train_acc.npy', 'rb') as f:\n",
    "    baseline_train_acc = np.load(f)\n",
    "with open('Base_corrupt_validation_acc.npy', 'rb') as f:\n",
    "    baseline_val_acc = np.load(f)\n",
    "with open('Base_source_acc.npy', 'rb') as f:\n",
    "    baseline_ref_acc = np.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8a047",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rpl_corrupt_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce230f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rpl_corrupt_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9851101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72146b31",
   "metadata": {},
   "source": [
    "## Comparison with TENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec5e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation accuracy averaged over all corruptions and all batch sizes\n",
    "# sev1, 2, 3, 4, 5\n",
    "rpl_corrupt_val_acc.transpose(2, 1, 0).mean(axis=0).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4339a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation accuracy (batch size = 32) averaged over all corruptions\n",
    "# sev1, 2, 3, 4, 5\n",
    "rpl_corrupt_val_acc.transpose(2, 1, 0)[0].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation error rate averaged over all corruptions and all batch sizes, shuffled\n",
    "# sev5, 4, 3, 2, 1\n",
    "(1 - rpl_corrupt_val_acc.transpose(2, 1, 0).mean(axis=0).mean(axis=1))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ccf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation error rate (batch size = 32) averaged over all corruptions, shuffled\n",
    "# sev5, 4, 3, 2, 1\n",
    "1 - rpl_corrupt_val_acc.transpose(2, 1, 0)[0].mean(axis=1)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb843ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training / online error rate averaged over all corruptions and all batch sizes, shuffled\n",
    "# sev5, 4, 3, 2, 1\n",
    "(1 - rpl_corrupt_train_acc.transpose(2, 1, 0).mean(axis=0).mean(axis=1))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a33003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline randomed error rate\n",
    "# sev5, 4, 3, 2, 1\n",
    "(1 - baseline_performances.transpose(2, 1, 0).mean(axis=0).mean(axis=1))[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabff35",
   "metadata": {},
   "source": [
    "## Comparison with RPL Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corruption)\n",
    "1-rpl_corrupt_val_acc.mean(axis=1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - baseline_performances.mean(axis=1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c83c1a",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1767bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ResNet50_performances.npy', 'rb') as f:\n",
    "    baseline_performances = np.load(f)\n",
    "with open('ResNet50_times.npy', 'rb') as f:\n",
    "    baseline_inf_times = np.load(f)\n",
    "# with open('RPL_corrupt_train_acc.npy', 'rb') as f:\n",
    "#     rpl_corrupt_train_acc = np.load(f)\n",
    "# with open('RPL_corrupt_validation_acc.npy', 'rb') as f:\n",
    "#     rpl_corrupt_val_acc = np.load(f)\n",
    "# with open('RPL_target_acc.npy', 'rb') as f:\n",
    "#     rpl_ref_acc = np.load(f)\n",
    "# with open('RPL_adapt_time.npy', 'rb') as f:\n",
    "#     rpl_adpt_times = np.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# severity 5, 16 graphs for 16 corruptions, \n",
    "# 6 lines for 6 batch sizes, an extra line for baseline corruption, an extra line for ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d245596",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sizes = [32, 64, 128, 256, 512]\n",
    "severity = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb8a104",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,4, figsize=(18, 18))\n",
    "fig.suptitle('Accuracy on corrupted (severity = 3) training set')\n",
    "for cor_ind in range(len(corruption)):\n",
    "    ax[cor_ind//4][cor_ind%4].plot(target_sizes, baseline_train_acc[cor_ind][2], label = 'Resnet50 acc train set')\n",
    "    ax[cor_ind//4][cor_ind%4].plot(target_sizes, rpl_corrupt_train_acc[cor_ind][2], label = 'Adapted train acc')\n",
    "    \n",
    "    ax[cor_ind//4][cor_ind%4].legend()\n",
    "    ax[cor_ind//4][cor_ind%4].set_title('Corruption: '+corruption[cor_ind])\n",
    "    ax[cor_ind//4][cor_ind%4].set_xticks(target_sizes)\n",
    "    ax[cor_ind//4][cor_ind%4].set_ylim([0, 0.7])\n",
    "\n",
    "\n",
    "    \n",
    "for a in ax.flat:\n",
    "    a.set(xlabel='Target adaptation batch size', ylabel='Accuracy')\n",
    "    \n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for a in ax.flat:\n",
    "    a.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43315f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,4, figsize=(18, 18))\n",
    "fig.suptitle('Accuracy on corrupted (severity = 3) validation set')\n",
    "for cor_ind in range(len(corruption)):\n",
    "    ax[cor_ind//4][cor_ind%4].plot(target_sizes, baseline_val_acc[cor_ind][2], label = 'Resnet50 val acc')\n",
    "    ax[cor_ind//4][cor_ind%4].plot(target_sizes, rpl_corrupt_val_acc[cor_ind][2], label='Adapted val acc')\n",
    "    \n",
    "    ax[cor_ind//4][cor_ind%4].legend()\n",
    "    ax[cor_ind//4][cor_ind%4].set_title('Corruption: '+corruption[cor_ind])\n",
    "    ax[cor_ind//4][cor_ind%4].set_xticks(target_sizes)\n",
    "    ax[cor_ind//4][cor_ind%4].set_ylim([0, 0.9])\n",
    "\n",
    "\n",
    "    \n",
    "for a in ax.flat:\n",
    "    a.set(xlabel='Target adaptation batch size', ylabel='Accuracy')\n",
    "    \n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for a in ax.flat:\n",
    "    a.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f01a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4,4, figsize=(18, 18))\n",
    "fig.suptitle('Accuracy on corrupted (severity = 3) train & validation set')\n",
    "for cor_ind in range(len(corruption)):\n",
    "    ax[cor_ind//4][cor_ind%4].plot(target_sizes, baseline_val_acc[cor_ind][2], label = 'Resnet50 val acc')\n",
    "    ax[cor_ind//4][cor_ind%4].plot(target_sizes, rpl_corrupt_val_acc[cor_ind][2], label='Adapted val acc')\n",
    "    ax[cor_ind//4][cor_ind%4].plot(target_sizes, baseline_performances[cor_ind][2][:-1], label = 'Resnet50_random')\n",
    "    \n",
    "    ax[cor_ind//4][cor_ind%4].plot(target_sizes, baseline_train_acc[cor_ind][2], label = 'Resnet50 train acc')\n",
    "    ax[cor_ind//4][cor_ind%4].plot(target_sizes, rpl_corrupt_train_acc[cor_ind][2], label = 'Adapted train acc')\n",
    "    \n",
    "    ax[cor_ind//4][cor_ind%4].legend()\n",
    "    ax[cor_ind//4][cor_ind%4].set_title('Corruption: '+corruption[cor_ind])\n",
    "    ax[cor_ind//4][cor_ind%4].set_xticks(target_sizes)\n",
    "    ax[cor_ind//4][cor_ind%4].set_ylim([0, 0.9])\n",
    "\n",
    "\n",
    "    \n",
    "for a in ax.flat:\n",
    "    a.set(xlabel='Target adaptation batch size', ylabel='Accuracy')\n",
    "    \n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for a in ax.flat:\n",
    "    a.label_outer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738b3342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0964739a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b9664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db5312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71bb0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5605bd14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2649aa9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d409aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e5497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d9dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08b987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
